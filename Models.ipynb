{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1234/1234 [==============================] - 2628s 2s/step - loss: 0.8465 - accuracy: 0.4468\n",
      "Epoch 2/20\n",
      "1234/1234 [==============================] - 2596s 2s/step - loss: 0.7028 - accuracy: 0.5389\n",
      "Epoch 3/20\n",
      "1234/1234 [==============================] - 2597s 2s/step - loss: 0.5907 - accuracy: 0.6128\n",
      "Epoch 4/20\n",
      "1234/1234 [==============================] - 2597s 2s/step - loss: 0.4941 - accuracy: 0.6786\n",
      "Epoch 5/20\n",
      "1234/1234 [==============================] - 2600s 2s/step - loss: 0.4092 - accuracy: 0.7433\n",
      "Epoch 6/20\n",
      "1234/1234 [==============================] - 2596s 2s/step - loss: 0.3458 - accuracy: 0.7851\n",
      "Epoch 7/20\n",
      "1234/1234 [==============================] - 2588s 2s/step - loss: 0.2983 - accuracy: 0.8178\n",
      "Epoch 8/20\n",
      "1234/1234 [==============================] - 2599s 2s/step - loss: 0.2588 - accuracy: 0.8432\n",
      "Epoch 9/20\n",
      "1234/1234 [==============================] - 2610s 2s/step - loss: 0.2279 - accuracy: 0.8620\n",
      "Epoch 10/20\n",
      "1234/1234 [==============================] - 2608s 2s/step - loss: 0.2107 - accuracy: 0.8749\n",
      "Epoch 11/20\n",
      "1234/1234 [==============================] - 2601s 2s/step - loss: 0.1936 - accuracy: 0.8857\n",
      "Epoch 12/20\n",
      "1234/1234 [==============================] - 2587s 2s/step - loss: 0.1777 - accuracy: 0.8948\n",
      "Epoch 13/20\n",
      "1234/1234 [==============================] - 2593s 2s/step - loss: 0.1667 - accuracy: 0.9016\n",
      "Epoch 14/20\n",
      "1234/1234 [==============================] - 2593s 2s/step - loss: 0.1574 - accuracy: 0.9092\n",
      "Epoch 15/20\n",
      "1234/1234 [==============================] - 2604s 2s/step - loss: 0.1486 - accuracy: 0.9134\n",
      "Epoch 16/20\n",
      "1234/1234 [==============================] - 2594s 2s/step - loss: 0.1428 - accuracy: 0.9169\n",
      "Epoch 17/20\n",
      "1234/1234 [==============================] - 2586s 2s/step - loss: 0.1353 - accuracy: 0.9206\n",
      "Epoch 18/20\n",
      "1234/1234 [==============================] - 2586s 2s/step - loss: 0.1293 - accuracy: 0.9251\n",
      "Epoch 19/20\n",
      "1234/1234 [==============================] - 2587s 2s/step - loss: 0.1230 - accuracy: 0.9286\n",
      "Epoch 20/20\n",
      "1234/1234 [==============================] - 2585s 2s/step - loss: 0.1214 - accuracy: 0.9301\n",
      "309/309 - 144s - loss: 0.1608 - accuracy: 0.9422 - 144s/epoch - 465ms/step\n",
      "Test Accuracy: 94.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "# Define your data directory, batch size, image size, number of epochs, and number of classes\n",
    "data_dir = r'D:\\Alzheimers\\Task1\\Task1-Images\\PET\\Images'\n",
    "batch_size = 32\n",
    "image_size = (128, 128)\n",
    "num_epochs = 20\n",
    "num_classes = 5\n",
    "\n",
    "def load_and_preprocess_images_and_labels(directory):\n",
    "    # Function to load and preprocess images and labels\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            dcm_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_path)\n",
    "                img = dcm_data.pixel_array.astype(np.float32) / 255.0\n",
    "                img = cv2.resize(img, image_size)\n",
    "                image_data.append(img)\n",
    "\n",
    "                filename_parts = filename.split('_')\n",
    "                class_name = filename_parts[-1].split('.')[0]\n",
    "\n",
    "                class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2, 'EMCI': 3, 'LMCI': 4}\n",
    "                label = class_mapping.get(class_name, -1)\n",
    "\n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Unknown class name: {class_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {dcm_path}: {str(e)}\")\n",
    "\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "# Load and preprocess all data from the directory\n",
    "all_images, all_labels = load_and_preprocess_images_and_labels(data_dir)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Expand grayscale images to three channels (RGB)\n",
    "train_images = np.repeat(train_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "test_images = np.repeat(test_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_sample_weight('balanced', train_labels.argmax(axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Create a data generator for training images with more augmentation\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load the EfficientNetB3 model\n",
    "base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))  # Use EfficientNetB3\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Add dropout for regularization\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze some layers of the base model (you can experiment with the number of layers to freeze)\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the data generator\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    history = model.fit(\n",
    "        train_data_generator.flow(train_images, train_labels, batch_size=batch_size),\n",
    "        steps_per_epoch=len(train_images) // batch_size,\n",
    "        epochs=1,\n",
    "        class_weight=class_weights,  # Apply class weights\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Define the path where you want to save the model\n",
    "save_path = r'D:\\Alzheimers\\Models\\PET_Task1.h5'  \n",
    "\n",
    "# Save the model to the specified location\n",
    "model.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "16019/16019 [==============================] - 2777s 173ms/step - loss: 0.5988 - accuracy: 0.7300\n",
      "Epoch 2/5\n",
      "16019/16019 [==============================] - 2845s 178ms/step - loss: 0.1452 - accuracy: 0.9457\n",
      "Epoch 3/5\n",
      "16019/16019 [==============================] - 2850s 178ms/step - loss: 0.0900 - accuracy: 0.9684\n",
      "Epoch 4/5\n",
      "16019/16019 [==============================] - 2836s 177ms/step - loss: 0.0680 - accuracy: 0.9770\n",
      "Epoch 5/5\n",
      "16019/16019 [==============================] - 2849s 178ms/step - loss: 0.0610 - accuracy: 0.9800\n",
      "1002/1002 - 257s - loss: 0.0117 - accuracy: 0.9968 - 257s/epoch - 257ms/step\n",
      "Test Accuracy: 99.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "data_dir = r'D:\\Alzheimers\\Task1\\Task1-Images\\fMRI\\Images'\n",
    "batch_size = 8\n",
    "image_size = (75, 75)\n",
    "num_epochs = 5  \n",
    "num_classes = 5  \n",
    "\n",
    "def load_and_preprocess_images_and_labels(directory):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            dcm_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_path)\n",
    "                img = dcm_data.pixel_array.astype(np.float32) / 255.0\n",
    "                img = cv2.resize(img, image_size)\n",
    "                image_data.append(img)\n",
    "                \n",
    "                filename_parts = filename.split('_')\n",
    "                class_name = filename_parts[-1].split('.')[0]\n",
    "                \n",
    "                class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2, 'EMCI': 3, 'LMCI': 4}\n",
    "                label = class_mapping.get(class_name, -1) \n",
    "                \n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Unknown class name: {class_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {dcm_path}: {str(e)}\")\n",
    "\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "\n",
    "# Load and preprocess all data from the directory\n",
    "all_images, all_labels = load_and_preprocess_images_and_labels(data_dir)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Expand grayscale images to three channels (RGB)\n",
    "train_images = np.repeat(train_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "test_images = np.repeat(test_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_sample_weight('balanced', train_labels.argmax(axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Create a data generator for training images with more augmentation\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load the VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(75, 75, 3))\n",
    "# or, alternatively, use InceptionV3\n",
    "# base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Add dropout for regularization\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze some layers of the base model (you can experiment with the number of layers to freeze)\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the data generator\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    history = model.fit(\n",
    "        train_data_generator.flow(train_images, train_labels, batch_size=batch_size),\n",
    "        steps_per_epoch=len(train_images) // batch_size,\n",
    "        epochs=1,\n",
    "        class_weight=class_weights,  # Apply class weights\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Define the path where you want to save the model\n",
    "save_path = r'D:\\Alzheimers\\Models\\fMRI_Task1.h5'  # Replace with your desired path\n",
    "\n",
    "# Save the model to the specified location\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528/528 [==============================] - 254s 475ms/step\n",
      "Test Accuracy: 89.12270302311796 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('D:/Alzheimers/Models/MRI_Task1.h5')  # Replace with the actual path to your saved model\n",
    "\n",
    "# Define data directory and image size\n",
    "data_dir = r'D:\\Alzheimers\\Task1\\Task1-Images\\MRI\\Images'\n",
    "image_size = (128, 128)  # Adjust this to match the input size your model expects\n",
    "\n",
    "# Create a function to load and preprocess test images\n",
    "def load_and_preprocess_test_images(directory):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            dcm_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_path)\n",
    "                img = dcm_data.pixel_array.astype(np.float32) / 255.0\n",
    "                img = cv2.resize(img, image_size)\n",
    "                image_data.append(img)\n",
    "                \n",
    "                filename_parts = filename.split('_')\n",
    "                class_name = filename_parts[-1].split('.')[0]\n",
    "                \n",
    "                class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2, 'EMCI': 3, 'LMCI': 4}\n",
    "                label = class_mapping.get(class_name, -1) \n",
    "                \n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Unknown class name: {class_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {dcm_path}: {str(e)}\")\n",
    "\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "# Load and preprocess the test images\n",
    "test_images, true_labels = load_and_preprocess_test_images(data_dir)\n",
    "\n",
    "# Expand grayscale images to three channels (RGB)\n",
    "test_images = np.repeat(test_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Use the model to make predictions on the test data\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "# Obtain predicted labels (class indices with highest probability)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy by comparing predicted and true labels\n",
    "accuracy = np.mean(np.equal(predicted_labels, true_labels))\n",
    "\n",
    "# Print the accuracy \n",
    "print(\"Test Accuracy:\", accuracy * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
