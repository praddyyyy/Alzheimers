{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "16019/16019 [==============================] - 3672s 229ms/step - loss: 0.4871 - accuracy: 0.7767\n",
      "Epoch 2/5\n",
      "16019/16019 [==============================] - 3670s 229ms/step - loss: 0.0874 - accuracy: 0.9684\n",
      "Epoch 3/5\n",
      "16019/16019 [==============================] - 3678s 230ms/step - loss: 0.0563 - accuracy: 0.9804\n",
      "Epoch 4/5\n",
      "16019/16019 [==============================] - 3658s 228ms/step - loss: 0.0447 - accuracy: 0.9852\n",
      "Epoch 5/5\n",
      "16019/16019 [==============================] - 3670s 229ms/step - loss: 0.0400 - accuracy: 0.9874\n",
      "1002/1002 - 407s - loss: 0.0038 - accuracy: 0.9989 - 407s/epoch - 407ms/step\n",
      "Test Accuracy: 99.89%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "data_dir = r'D:\\Alzheimers\\Task1\\Task1-Images\\fMRI\\Images'\n",
    "batch_size = 8\n",
    "image_size = (96, 96)\n",
    "num_epochs = 5  \n",
    "num_classes = 5  \n",
    "\n",
    "def load_and_preprocess_images_and_labels(directory):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            dcm_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_path)\n",
    "                img = dcm_data.pixel_array.astype(np.float32) / 255.0\n",
    "                img = cv2.resize(img, image_size)\n",
    "                image_data.append(img)\n",
    "                \n",
    "                filename_parts = filename.split('_')\n",
    "                class_name = filename_parts[-1].split('.')[0]\n",
    "                \n",
    "                class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2, 'EMCI': 3, 'LMCI': 4}\n",
    "                label = class_mapping.get(class_name, -1) \n",
    "                \n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Unknown class name: {class_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {dcm_path}: {str(e)}\")\n",
    "\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "\n",
    "# Load and preprocess all data from the directory\n",
    "all_images, all_labels = load_and_preprocess_images_and_labels(data_dir)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Expand grayscale images to three channels (RGB)\n",
    "train_images = np.repeat(train_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "test_images = np.repeat(test_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_sample_weight('balanced', train_labels.argmax(axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Create a data generator for training images with more augmentation\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load the VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "# or, alternatively, use InceptionV3\n",
    "# base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Add dropout for regularization\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze some layers of the base model (you can experiment with the number of layers to freeze)\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the data generator\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    history = model.fit(\n",
    "        train_data_generator.flow(train_images, train_labels, batch_size=batch_size),\n",
    "        steps_per_epoch=len(train_images) // batch_size,\n",
    "        epochs=1,\n",
    "        class_weight=class_weights,  # Apply class weights\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Define the path where you want to save the model\n",
    "save_path = r'D:\\Alzheimers\\Models\\fMRI_Task1.h5'  # Replace with your desired path\n",
    "\n",
    "# Save the model to the specified location\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\Alzheimers\\Task1\\Task1-Images\\fMRI\\Model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\Alzheimers\\Task1\\Task1-Images\\fMRI\\Model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the path where you want to save the model\n",
    "model_save_path = r'D:\\Alzheimers\\Task1\\Task1-Images\\fMRI\\Model'  # Change this path as needed\n",
    "\n",
    "# Save the trained model\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 8s 0us/step\n",
      "Epoch 1/5\n",
      "16019/16019 [==============================] - 5067s 315ms/step - loss: 1.2420 - accuracy: 0.3803\n",
      "Epoch 2/5\n",
      "16019/16019 [==============================] - 5911s 369ms/step - loss: 0.8882 - accuracy: 0.5893\n",
      "Epoch 3/5\n",
      "16019/16019 [==============================] - 4798s 300ms/step - loss: 0.5311 - accuracy: 0.7794\n",
      "Epoch 4/5\n",
      "16019/16019 [==============================] - 4733s 295ms/step - loss: 0.3546 - accuracy: 0.8626\n",
      "Epoch 5/5\n",
      "16019/16019 [==============================] - 4717s 294ms/step - loss: 0.2522 - accuracy: 0.9060\n",
      "1002/1002 - 108s - loss: 0.5545 - accuracy: 0.9644 - 108s/epoch - 107ms/step\n",
      "Test Accuracy: 96.44%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "data_dir = r'D:\\Alzheimers\\Task1\\Task1-Images\\fMRI\\Images'\n",
    "batch_size = 8\n",
    "image_size = (96, 96)\n",
    "num_epochs = 5\n",
    "num_classes = 5\n",
    "\n",
    "def load_and_preprocess_images_and_labels(directory):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            dcm_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_path)\n",
    "                img = dcm_data.pixel_array.astype(np.float32) / 255.0\n",
    "                img = cv2.resize(img, image_size)\n",
    "                image_data.append(img)\n",
    "\n",
    "                filename_parts = filename.split('_')\n",
    "                class_name = filename_parts[-1].split('.')[0]\n",
    "\n",
    "                class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2, 'EMCI': 3, 'LMCI': 4}\n",
    "                label = class_mapping.get(class_name, -1)\n",
    "\n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Unknown class name: {class_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {dcm_path}: {str(e)}\")\n",
    "\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "# Load and preprocess all data from the directory\n",
    "all_images, all_labels = load_and_preprocess_images_and_labels(data_dir)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Expand grayscale images to three channels (RGB)\n",
    "train_images = np.repeat(train_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "test_images = np.repeat(test_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_sample_weight('balanced', train_labels.argmax(axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Create a data generator for training images with more augmentation\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load the InceptionV3 model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Add dropout for regularization\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze some layers of the base model (you can experiment with the number of layers to freeze)\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the data generator\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    history = model.fit(\n",
    "        train_data_generator.flow(train_images, train_labels, batch_size=batch_size),\n",
    "        steps_per_epoch=len(train_images) // batch_size,\n",
    "        epochs=1,\n",
    "        class_weight=class_weights,  # Apply class weights\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "421/421 [==============================] - 1174s 3s/step - loss: 1.3600 - accuracy: 0.3152\n",
      "Epoch 2/5\n",
      "421/421 [==============================] - 1159s 3s/step - loss: 1.3128 - accuracy: 0.3422\n",
      "Epoch 3/5\n",
      "102/421 [======>.......................] - ETA: 14:33 - loss: 1.2985 - accuracy: 0.3498"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Alzheimers\\Code\\task1-test.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#W6sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#W6sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#W6sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#W6sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m         train_data_generator\u001b[39m.\u001b[39;49mflow(train_images, train_labels, batch_size\u001b[39m=\u001b[39;49mbatch_size),\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#W6sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m         steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(train_images) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m batch_size,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#W6sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m         epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#W6sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m         class_weight\u001b[39m=\u001b[39;49mclass_weights,  \u001b[39m# Apply class weights\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#W6sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#W6sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     )\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#W6sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39m# Evaluate the model on the test dataset\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#W6sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m test_loss, test_accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(test_images, test_labels, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "data_dir = r'D:\\Alzheimers\\Task1\\Task1-Images\\MRI\\Images'\n",
    "batch_size = 32\n",
    "image_size = (224, 224)\n",
    "num_epochs = 5\n",
    "num_classes = 5\n",
    "\n",
    "def load_and_preprocess_images_and_labels(directory):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            dcm_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_path)\n",
    "                img = dcm_data.pixel_array.astype(np.float32) / 255.0\n",
    "                img = cv2.resize(img, image_size)\n",
    "                image_data.append(img)\n",
    "\n",
    "                filename_parts = filename.split('_')\n",
    "                class_name = filename_parts[-1].split('.')[0]\n",
    "\n",
    "                class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2, 'EMCI': 3, 'LMCI': 4}\n",
    "                label = class_mapping.get(class_name, -1)\n",
    "\n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Unknown class name: {class_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {dcm_path}: {str(e)}\")\n",
    "\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "# Load and preprocess all data from the directory\n",
    "all_images, all_labels = load_and_preprocess_images_and_labels(data_dir)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Expand grayscale images to three channels (RGB)\n",
    "train_images = np.repeat(train_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "test_images = np.repeat(test_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_sample_weight('balanced', train_labels.argmax(axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Create a data generator for training images with more augmentation\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load the InceptionV3 model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Add dropout for regularization\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze some layers of the base model (you can experiment with the number of layers to freeze)\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the data generator\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    history = model.fit(\n",
    "        train_data_generator.flow(train_images, train_labels, batch_size=batch_size),\n",
    "        steps_per_epoch=len(train_images) // batch_size,\n",
    "        epochs=1,\n",
    "        class_weight=class_weights,  # Apply class weights\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1234/1234 [==============================] - 1316s 1s/step - loss: 0.8454 - accuracy: 0.4485\n",
      "Epoch 2/20\n",
      "1234/1234 [==============================] - 1327s 1s/step - loss: 0.7213 - accuracy: 0.5187\n",
      "Epoch 3/20\n",
      "1234/1234 [==============================] - 1317s 1s/step - loss: 0.6448 - accuracy: 0.5687\n",
      "Epoch 4/20\n",
      "1234/1234 [==============================] - 1313s 1s/step - loss: 0.5765 - accuracy: 0.6185\n",
      "Epoch 5/20\n",
      "1234/1234 [==============================] - 1316s 1s/step - loss: 0.5117 - accuracy: 0.6628\n",
      "Epoch 6/20\n",
      "1234/1234 [==============================] - 1315s 1s/step - loss: 0.4610 - accuracy: 0.6995\n",
      "Epoch 7/20\n",
      "1234/1234 [==============================] - 1317s 1s/step - loss: 0.4099 - accuracy: 0.7356\n",
      "Epoch 8/20\n",
      "1234/1234 [==============================] - 1320s 1s/step - loss: 0.3682 - accuracy: 0.7643\n",
      "Epoch 9/20\n",
      "1234/1234 [==============================] - 1319s 1s/step - loss: 0.3338 - accuracy: 0.7894\n",
      "Epoch 10/20\n",
      "1234/1234 [==============================] - 1320s 1s/step - loss: 0.3054 - accuracy: 0.8085\n",
      "Epoch 11/20\n",
      "1234/1234 [==============================] - 1315s 1s/step - loss: 0.2799 - accuracy: 0.8247\n",
      "Epoch 12/20\n",
      "1234/1234 [==============================] - 1318s 1s/step - loss: 0.2579 - accuracy: 0.8417\n",
      "Epoch 13/20\n",
      "1234/1234 [==============================] - 1321s 1s/step - loss: 0.2391 - accuracy: 0.8536\n",
      "Epoch 14/20\n",
      "1234/1234 [==============================] - 1319s 1s/step - loss: 0.2230 - accuracy: 0.8633\n",
      "Epoch 15/20\n",
      "1234/1234 [==============================] - 1327s 1s/step - loss: 0.2135 - accuracy: 0.8709\n",
      "Epoch 16/20\n",
      "1234/1234 [==============================] - 1316s 1s/step - loss: 0.1996 - accuracy: 0.8781\n",
      "Epoch 17/20\n",
      "1234/1234 [==============================] - 1316s 1s/step - loss: 0.1900 - accuracy: 0.8863\n",
      "Epoch 18/20\n",
      "1234/1234 [==============================] - 1318s 1s/step - loss: 0.1804 - accuracy: 0.8905\n",
      "Epoch 19/20\n",
      "1234/1234 [==============================] - 1335s 1s/step - loss: 0.1718 - accuracy: 0.8970\n",
      "Epoch 20/20\n",
      "1234/1234 [==============================] - 1319s 1s/step - loss: 0.1643 - accuracy: 0.8995\n",
      "309/309 - 226s - loss: 0.2530 - accuracy: 0.9022 - 226s/epoch - 732ms/step\n",
      "Test Accuracy: 90.22%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "data_dir = r'D:\\Alzheimers\\Task1\\Task1-Images\\PET\\Images'\n",
    "batch_size = 32\n",
    "image_size = (128, 128)\n",
    "num_epochs = 20\n",
    "num_classes = 5  \n",
    "\n",
    "def load_and_preprocess_images_and_labels(directory):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            dcm_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_path)\n",
    "                img = dcm_data.pixel_array.astype(np.float32) / 255.0\n",
    "                img = cv2.resize(img, image_size)\n",
    "                image_data.append(img)\n",
    "                \n",
    "                filename_parts = filename.split('_')\n",
    "                class_name = filename_parts[-1].split('.')[0]\n",
    "                \n",
    "                class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2, 'EMCI': 3, 'LMCI': 4}\n",
    "                label = class_mapping.get(class_name, -1) \n",
    "                \n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Unknown class name: {class_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {dcm_path}: {str(e)}\")\n",
    "\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "\n",
    "# Load and preprocess all data from the directory\n",
    "all_images, all_labels = load_and_preprocess_images_and_labels(data_dir)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Expand grayscale images to three channels (RGB)\n",
    "train_images = np.repeat(train_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "test_images = np.repeat(test_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_sample_weight('balanced', train_labels.argmax(axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Create a data generator for training images with more augmentation\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load the VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "# or, alternatively, use InceptionV3\n",
    "# base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Add dropout for regularization\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze some layers of the base model (you can experiment with the number of layers to freeze)\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the data generator\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    history = model.fit(\n",
    "        train_data_generator.flow(train_images, train_labels, batch_size=batch_size),\n",
    "        steps_per_epoch=len(train_images) // batch_size,\n",
    "        epochs=1,\n",
    "        class_weight=class_weights,  # Apply class weights\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1234/1234 [==============================] - 2637s 2s/step - loss: 0.8447 - accuracy: 0.4454\n",
      "Epoch 2/20\n",
      "1234/1234 [==============================] - 2617s 2s/step - loss: 0.6978 - accuracy: 0.5397\n",
      "Epoch 3/20\n",
      "1234/1234 [==============================] - 2596s 2s/step - loss: 0.5840 - accuracy: 0.6156\n",
      "Epoch 4/20\n",
      "1234/1234 [==============================] - 2601s 2s/step - loss: 0.4834 - accuracy: 0.6883\n",
      "Epoch 5/20\n",
      "1234/1234 [==============================] - 2607s 2s/step - loss: 0.4065 - accuracy: 0.7415\n",
      "Epoch 6/20\n",
      "1234/1234 [==============================] - 2609s 2s/step - loss: 0.3403 - accuracy: 0.7875\n",
      "Epoch 7/20\n",
      "1234/1234 [==============================] - 2612s 2s/step - loss: 0.2936 - accuracy: 0.8203\n",
      "Epoch 8/20\n",
      "1234/1234 [==============================] - 2597s 2s/step - loss: 0.2589 - accuracy: 0.8438\n",
      "Epoch 9/20\n",
      "1234/1234 [==============================] - 2613s 2s/step - loss: 0.2311 - accuracy: 0.8602\n",
      "Epoch 10/20\n",
      "1234/1234 [==============================] - 2610s 2s/step - loss: 0.2112 - accuracy: 0.8727\n",
      "Epoch 11/20\n",
      "1234/1234 [==============================] - 2607s 2s/step - loss: 0.1906 - accuracy: 0.8867\n",
      "Epoch 12/20\n",
      "1234/1234 [==============================] - 2596s 2s/step - loss: 0.1780 - accuracy: 0.8956\n",
      "Epoch 13/20\n",
      "1234/1234 [==============================] - 2592s 2s/step - loss: 0.1650 - accuracy: 0.9042\n",
      "Epoch 14/20\n",
      "1234/1234 [==============================] - 2636s 2s/step - loss: 0.1548 - accuracy: 0.9094\n",
      "Epoch 15/20\n",
      "1234/1234 [==============================] - 2602s 2s/step - loss: 0.1498 - accuracy: 0.9126\n",
      "Epoch 16/20\n",
      "1234/1234 [==============================] - 2605s 2s/step - loss: 0.1367 - accuracy: 0.9191\n",
      "Epoch 17/20\n",
      "1234/1234 [==============================] - 3012s 2s/step - loss: 0.1288 - accuracy: 0.9242\n",
      "Epoch 18/20\n",
      "1234/1234 [==============================] - 2810s 2s/step - loss: 0.1288 - accuracy: 0.9251\n",
      "Epoch 19/20\n",
      "1234/1234 [==============================] - 8225s 7s/step - loss: 0.1235 - accuracy: 0.9274\n",
      "Epoch 20/20\n",
      "1234/1234 [==============================] - 2610s 2s/step - loss: 0.1161 - accuracy: 0.9325\n",
      "309/309 - 145s - loss: 0.1762 - accuracy: 0.9370 - 145s/epoch - 470ms/step\n",
      "Test Accuracy: 93.70%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB3  # Import EfficientNetB3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "data_dir = r'D:\\Alzheimers\\Task1\\Task1-Images\\PET\\Images'\n",
    "batch_size = 32\n",
    "image_size = (128, 128)\n",
    "num_epochs = 20\n",
    "num_classes = 5  \n",
    "\n",
    "def load_and_preprocess_images_and_labels(directory):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            dcm_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_path)\n",
    "                img = dcm_data.pixel_array.astype(np.float32) / 255.0\n",
    "                img = cv2.resize(img, image_size)\n",
    "                image_data.append(img)\n",
    "                \n",
    "                filename_parts = filename.split('_')\n",
    "                class_name = filename_parts[-1].split('.')[0]\n",
    "                \n",
    "                class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2, 'EMCI': 3, 'LMCI': 4}\n",
    "                label = class_mapping.get(class_name, -1) \n",
    "                \n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Unknown class name: {class_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {dcm_path}: {str(e)}\")\n",
    "\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "# Load and preprocess all data from the directory\n",
    "all_images, all_labels = load_and_preprocess_images_and_labels(data_dir)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Expand grayscale images to three channels (RGB)\n",
    "train_images = np.repeat(train_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "test_images = np.repeat(test_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_sample_weight('balanced', train_labels.argmax(axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Create a data generator for training images with more augmentation\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load the EfficientNetB3 model\n",
    "base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))  # Use EfficientNetB3\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Add dropout for regularization\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze some layers of the base model (you can experiment with the number of layers to freeze)\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the data generator\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    history = model.fit(\n",
    "        train_data_generator.flow(train_images, train_labels, batch_size=batch_size),\n",
    "        steps_per_epoch=len(train_images) // batch_size,\n",
    "        epochs=1,\n",
    "        class_weight=class_weights,  # Apply class weights\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 Complete [01h 45m 13s]\n",
      "val_accuracy: 0.6600000262260437\n",
      "\n",
      "Best val_accuracy So Far: 0.7237036824226379\n",
      "Total elapsed time: 16h 21m 40s\n",
      "\n",
      "Search: Running Trial #8\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "EfficientNetB3    |EfficientNetB3    |transfer_model\n",
      "8.372e-05         |0.00019545        |regularization\n",
      "0.4               |0.3               |dropout\n",
      "2.9132e-05        |8.2548e-05        |learning_rate\n",
      "\n",
      "Epoch 1/20\n",
      "338/338 [==============================] - 792s 2s/step - loss: 1.5001 - accuracy: 0.3043 - val_loss: 1.6089 - val_accuracy: 0.3204\n",
      "Epoch 2/20\n",
      "338/338 [==============================] - 756s 2s/step - loss: 1.4220 - accuracy: 0.3383 - val_loss: 1.5390 - val_accuracy: 0.3441\n",
      "Epoch 3/20\n",
      "338/338 [==============================] - 755s 2s/step - loss: 1.3660 - accuracy: 0.3662 - val_loss: 1.4821 - val_accuracy: 0.3781\n",
      "Epoch 4/20\n",
      "338/338 [==============================] - 757s 2s/step - loss: 1.3089 - accuracy: 0.4054 - val_loss: 1.4511 - val_accuracy: 0.4104\n",
      "Epoch 5/20\n",
      "338/338 [==============================] - 755s 2s/step - loss: 1.2415 - accuracy: 0.4494 - val_loss: 1.4190 - val_accuracy: 0.4322\n",
      "Epoch 6/20\n",
      "338/338 [==============================] - 756s 2s/step - loss: 1.1688 - accuracy: 0.4906 - val_loss: 1.3723 - val_accuracy: 0.4485\n",
      "Epoch 7/20\n",
      " 44/338 [==>...........................] - ETA: 10:31 - loss: 1.0826 - accuracy: 0.5447"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Alzheimers\\Code\\task1-test.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m tuner \u001b[39m=\u001b[39m RandomSearch(\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m     build_model,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m     objective\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m     project_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39malzheimer\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# Name of the project\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39m# Search for the best hyperparameters\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m tuner\u001b[39m.\u001b[39;49msearch(train_images, train_labels, epochs\u001b[39m=\u001b[39;49mnum_epochs, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, class_weight\u001b[39m=\u001b[39;49mclass_weights, callbacks\u001b[39m=\u001b[39;49m[early_stopping])\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m \u001b[39m# Get the best model\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m best_model \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mget_best_models(num_models\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:230\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 230\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_run_and_update_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[0;32m    231\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:270\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[0;32m    269\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_and_update_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[0;32m    271\u001b[0m         trial\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m trial_module\u001b[39m.\u001b[39mTrialStatus\u001b[39m.\u001b[39mCOMPLETED\n\u001b[0;32m    272\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:235\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[1;32m--> 235\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[0;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mget_trial(trial\u001b[39m.\u001b[39mtrial_id)\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mexists(\n\u001b[0;32m    237\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective\u001b[39m.\u001b[39mname\n\u001b[0;32m    238\u001b[0m     ):\n\u001b[0;32m    239\u001b[0m         \u001b[39m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[0;32m    240\u001b[0m         \u001b[39m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[0;32m    241\u001b[0m         \u001b[39m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    243\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe use case of calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m    251\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras_tuner\\src\\engine\\tuner.py:307\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    306\u001b[0m     copied_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m callbacks\n\u001b[1;32m--> 307\u001b[0m     obj_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_and_fit_model(trial, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcopied_kwargs)\n\u001b[0;32m    309\u001b[0m     histories\u001b[39m.\u001b[39mappend(obj_value)\n\u001b[0;32m    310\u001b[0m \u001b[39mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras_tuner\\src\\engine\\tuner.py:227\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m hp \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39mhyperparameters\n\u001b[0;32m    226\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 227\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhypermodel\u001b[39m.\u001b[39;49mfit(hp, model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    229\u001b[0m \u001b[39m# Save the build config for model loading later.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m utils\u001b[39m.\u001b[39msave_json(\n\u001b[0;32m    231\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_build_config_fname(trial\u001b[39m.\u001b[39mtrial_id),\n\u001b[0;32m    232\u001b[0m     model\u001b[39m.\u001b[39mget_build_config(),\n\u001b[0;32m    233\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:144\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, hp, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    121\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Train the model.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \n\u001b[0;32m    123\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mfit(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.applications import VGG16, InceptionV3, EfficientNetB3\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import pydicom\n",
    "import cv2\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "import kerastuner\n",
    "\n",
    "\n",
    "# Set your data directory, batch size, image size, number of epochs, and number of classes\n",
    "data_dir = r'D:\\Alzheimers\\Task1\\Task1-Images\\MRI\\Images'\n",
    "batch_size = 32\n",
    "image_size = (128, 128)\n",
    "num_epochs = 20\n",
    "num_classes = 5\n",
    "\n",
    "# Function to load and preprocess images and labels\n",
    "def load_and_preprocess_images_and_labels(directory):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            dcm_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_path)\n",
    "                img = dcm_data.pixel_array.astype(np.float32) / 255.0\n",
    "                img = cv2.resize(img, image_size)\n",
    "                image_data.append(img)\n",
    "                \n",
    "                filename_parts = filename.split('_')\n",
    "                class_name = filename_parts[-1].split('.')[0]\n",
    "                \n",
    "                class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2, 'EMCI': 3, 'LMCI': 4}\n",
    "                label = class_mapping.get(class_name, -1) \n",
    "                \n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Unknown class name: {class_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {dcm_path}: {str(e)}\")\n",
    "\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "# Load and preprocess all data from the directory\n",
    "all_images, all_labels = load_and_preprocess_images_and_labels(data_dir)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Expand grayscale images to three channels (RGB)\n",
    "train_images = np.repeat(train_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "test_images = np.repeat(test_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_sample_weight('balanced', train_labels.argmax(axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Create a data generator for training images with more augmentation\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Define a function to create and compile a model\n",
    "def build_model(hp):\n",
    "    # Choose a transfer learning model architecture\n",
    "    transfer_model = hp.Choice('transfer_model', ['InceptionV3', 'EfficientNetB3'])\n",
    "    if transfer_model == 'InceptionV3':\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "    elif transfer_model == 'EfficientNetB3':\n",
    "        base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Add regularization (L2 regularization)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=l2(hp.Float('regularization', 1e-5, 1e-3, sampling='log')))(x)\n",
    "    \n",
    "    # Add dropout for regularization\n",
    "    x = Dropout(hp.Float('dropout', 0.2, 0.5, step=0.1))(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Freeze some layers of the base model\n",
    "    for layer in base_model.layers[:15]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Compile the model with a variable learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-5, 1e-2, sampling='log')),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Instantiate the RandomSearch tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Number of hyperparameter combinations to try\n",
    "    directory='tuner_results',  # Directory to store results\n",
    "    project_name='alzheimer'  # Name of the project\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(train_images, train_labels, epochs=num_epochs, validation_split=0.2, class_weight=class_weights, callbacks=[early_stopping])\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Train the best model with early stopping\n",
    "best_model.fit(\n",
    "    train_data_generator.flow(train_images, train_labels, batch_size=batch_size),\n",
    "    steps_per_epoch=len(train_images) // batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    class_weight=class_weights,  # Apply class weights\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the best model on the test dataset\n",
    "test_loss, test_accuracy = best_model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your three saved models (MRI, fMRI, and PET)\n",
    "# You'll need to replace these with the actual paths to your saved models.\n",
    "path_to_mri_model = 'path_to_saved_mri_model.pkl'\n",
    "path_to_fmri_model = 'path_to_saved_fmri_model.pkl'\n",
    "path_to_pet_model = 'path_to_saved_pet_model.pkl'\n",
    "\n",
    "# Load your data (features and labels)\n",
    "# Replace X and y with your actual data.\n",
    "X, y = load_data()  # You need to implement load_data() based on your dataset.\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the saved models\n",
    "mri_model = load_model(path_to_mri_model)  # You need to implement load_model() based on your model saving method.\n",
    "fmri_model = load_model(path_to_fmri_model)\n",
    "pet_model = load_model(path_to_pet_model)\n",
    "\n",
    "# Create a list of tuples where each tuple contains a name for the model and the model itself\n",
    "# You can use any name you prefer for each model.\n",
    "estimators = [\n",
    "    ('mri', mri_model),\n",
    "    ('fmri', fmri_model),\n",
    "    ('pet', pet_model)\n",
    "]\n",
    "\n",
    "# Create a VotingClassifier with majority voting\n",
    "voting_classifier = VotingClassifier(estimators=estimators, voting='hard')\n",
    "\n",
    "# Fit the VotingClassifier on the training data\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Voting Classifier Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Local\\Temp\\ipykernel_6068\\912037774.py:15: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "InceptionV3       |InceptionV3       |transfer_model\n",
      "2.4101e-05        |2.4101e-05        |regularization\n",
      "0.3               |0.3               |dropout\n",
      "1.7359e-05        |1.7359e-05        |learning_rate\n",
      "\n",
      "Epoch 1/20\n",
      " 73/338 [=====>........................] - ETA: 4:15 - loss: 1.5099 - accuracy: 0.2868"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Alzheimers\\Code\\task1-test.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m tuner \u001b[39m=\u001b[39m RandomSearch(\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m     build_model,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m     objective\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m     project_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39malzheimer\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# Name of the project\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m \u001b[39m# Search for the best hyperparameters\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m tuner\u001b[39m.\u001b[39;49msearch(train_images, train_labels, epochs\u001b[39m=\u001b[39;49mnum_epochs, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, class_weight\u001b[39m=\u001b[39;49mclass_weights, callbacks\u001b[39m=\u001b[39;49m[early_stopping, model_checkpoint_callback])\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m \u001b[39m# Get the best model\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Alzheimers/Code/task1-test.ipynb#X11sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m best_model \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mget_best_models(num_models\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:230\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 230\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_run_and_update_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[0;32m    231\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:270\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[0;32m    269\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_and_update_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[0;32m    271\u001b[0m         trial\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m trial_module\u001b[39m.\u001b[39mTrialStatus\u001b[39m.\u001b[39mCOMPLETED\n\u001b[0;32m    272\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:235\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[1;32m--> 235\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[0;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mget_trial(trial\u001b[39m.\u001b[39mtrial_id)\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mexists(\n\u001b[0;32m    237\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective\u001b[39m.\u001b[39mname\n\u001b[0;32m    238\u001b[0m     ):\n\u001b[0;32m    239\u001b[0m         \u001b[39m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[0;32m    240\u001b[0m         \u001b[39m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[0;32m    241\u001b[0m         \u001b[39m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    243\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe use case of calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m    251\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras_tuner\\src\\engine\\tuner.py:307\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    306\u001b[0m     copied_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m callbacks\n\u001b[1;32m--> 307\u001b[0m     obj_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_and_fit_model(trial, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcopied_kwargs)\n\u001b[0;32m    309\u001b[0m     histories\u001b[39m.\u001b[39mappend(obj_value)\n\u001b[0;32m    310\u001b[0m \u001b[39mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras_tuner\\src\\engine\\tuner.py:227\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m hp \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39mhyperparameters\n\u001b[0;32m    226\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 227\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhypermodel\u001b[39m.\u001b[39;49mfit(hp, model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    229\u001b[0m \u001b[39m# Save the build config for model loading later.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m utils\u001b[39m.\u001b[39msave_json(\n\u001b[0;32m    231\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_build_config_fname(trial\u001b[39m.\u001b[39mtrial_id),\n\u001b[0;32m    232\u001b[0m     model\u001b[39m.\u001b[39mget_build_config(),\n\u001b[0;32m    233\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:144\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, hp, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    121\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Train the model.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \n\u001b[0;32m    123\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mfit(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.applications import InceptionV3, EfficientNetB3\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import pydicom\n",
    "import cv2\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# Set your data directory, batch size, image size, number of epochs, and number of classes\n",
    "data_dir = r'D:\\Alzheimers\\Task1\\Task1-Images\\MRI\\Images'\n",
    "batch_size = 32\n",
    "image_size = (128, 128)\n",
    "num_epochs = 20\n",
    "num_classes = 5\n",
    "\n",
    "# Function to load and preprocess images and labels\n",
    "def load_and_preprocess_images_and_labels(directory):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            dcm_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_path)\n",
    "                img = dcm_data.pixel_array.astype(np.float32) / 255.0\n",
    "                img = cv2.resize(img, image_size)\n",
    "                image_data.append(img)\n",
    "                \n",
    "                filename_parts = filename.split('_')\n",
    "                class_name = filename_parts[-1].split('.')[0]\n",
    "                \n",
    "                class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2, 'EMCI': 3, 'LMCI': 4}\n",
    "                label = class_mapping.get(class_name, -1) \n",
    "                \n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Unknown class name: {class_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {dcm_path}: {str(e)}\")\n",
    "\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "# Load and preprocess all data from the directory\n",
    "all_images, all_labels = load_and_preprocess_images_and_labels(data_dir)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Expand grayscale images to three channels (RGB)\n",
    "train_images = np.repeat(train_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "test_images = np.repeat(test_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_sample_weight('balanced', train_labels.argmax(axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Create a data generator for training images with more augmentation\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Define a function to create and compile a model\n",
    "def build_model(hp):\n",
    "    # Choose a transfer learning model architecture\n",
    "    transfer_model = hp.Choice('transfer_model', ['InceptionV3', 'EfficientNetB3'])\n",
    "    if transfer_model == 'InceptionV3':\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "    elif transfer_model == 'EfficientNetB3':\n",
    "        base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Add regularization (L2 regularization)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=l2(hp.Float('regularization', 1e-5, 1e-3, sampling='log')))(x)\n",
    "    \n",
    "    # Add dropout for regularization\n",
    "    x = Dropout(hp.Float('dropout', 0.2, 0.5, step=0.1))(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Freeze some layers of the base model\n",
    "    for layer in base_model.layers[:15]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Compile the model with a variable learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-5, 1e-2, sampling='log')),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Define a ModelCheckpoint callback to save the best model\n",
    "checkpoint_filepath = 'MRI_Task1.h5'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_best_only=True,\n",
    "    monitor='val_accuracy',  # Monitor validation accuracy\n",
    "    mode='max',  # Save the highest accuracy models\n",
    "    verbose=1  # Display a message when the best model is saved\n",
    ")\n",
    "\n",
    "# Instantiate the RandomSearch tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Number of hyperparameter combinations to try\n",
    "    directory='tuner_results',  # Directory to store results\n",
    "    project_name='alzheimer'  # Name of the project\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(train_images, train_labels, epochs=num_epochs, validation_split=0.2, class_weight=class_weights, callbacks=[early_stopping, model_checkpoint_callback])\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate the best model on the test dataset\n",
    "test_loss, test_accuracy = best_model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
